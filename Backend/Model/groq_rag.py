import json
import os
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.llms.groq import Groq
from dotenv import load_dotenv
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Charger les variables d'environnement
load_dotenv()

# Configuration des API keys
GROQ_API_KEY = os.getenv("GROQ_API_KEY")

def load_scraped_data(json_file=None):
    """
    Charge les donn√©es scrap√©es depuis le fichier JSON.
    """
    if json_file is None:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        json_file = os.path.join(script_dir, '../data/scraped_data.json')
    if not os.path.exists(json_file):
        print(f"Fichier {json_file} non trouv√©.")
        return []
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def create_documents(data):
    """
    Cr√©e des documents LlamaIndex √† partir des donn√©es scrap√©es.
    """
    documents = []
    for item in data:
        doc = Document(
            text=item['content'],
            metadata={
                'url': item['url'],
                'timestamp': item['timestamp']
            }
        )
        documents.append(doc)
    return documents

class CustomEvaluator:
    """
    Classe d'√©valuation personnalis√©e pour √©valuer le RAG.
    """
    def __init__(self, embed_model):
        self.embed_model = embed_model
    
    def get_embedding(self, text):
        """Obtient l'embedding d'un texte."""
        try:
            embedding = self.embed_model.get_text_embedding(text)
            return np.array(embedding)
        except Exception as e:
            print(f"Erreur lors de l'embedding : {e}")
            return None
    
    def answer_relevancy(self, question, answer):
        """
        Mesure la pertinence de la r√©ponse par rapport √† la question.
        Score entre 0 et 1 (plus proche de 1 = plus pertinent).
        """
        try:
            q_embedding = self.get_embedding(question)
            a_embedding = self.get_embedding(answer)
            
            if q_embedding is None or a_embedding is None:
                return 0.0
            
            # Calcul de similarit√© cosinus
            similarity = cosine_similarity(
                [q_embedding],
                [a_embedding]
            )[0][0]
            
            # Normaliser entre 0 et 1
            score = (similarity + 1) / 2
            return float(score)
        except Exception as e:
            print(f"Erreur dans answer_relevancy : {e}")
            return 0.0
    
    def context_precision(self, question, contexts):
        """
        Mesure la pr√©cision du contexte.
        √âvalue si les contextes sont pertinents par rapport √† la question.
        Score entre 0 et 1.
        """
        try:
            if not contexts:
                return 0.0
            
            q_embedding = self.get_embedding(question)
            if q_embedding is None:
                return 0.0
            
            scores = []
            for context in contexts:
                c_embedding = self.get_embedding(context)
                if c_embedding is not None:
                    similarity = cosine_similarity(
                        [q_embedding],
                        [c_embedding]
                    )[0][0]
                    scores.append((similarity + 1) / 2)
            
            # Retourner la moyenne des scores
            if scores:
                return float(np.mean(scores))
            return 0.0
        except Exception as e:
            print(f"Erreur dans context_precision : {e}")
            return 0.0
    
    def context_recall(self, answer, contexts):
        """
        Mesure le rappel du contexte.
        √âvalue si la r√©ponse couvre les informations des contextes.
        Score entre 0 et 1.
        """
        try:
            if not contexts:
                return 0.0
            
            a_embedding = self.get_embedding(answer)
            if a_embedding is None:
                return 0.0
            
            scores = []
            for context in contexts:
                c_embedding = self.get_embedding(context)
                if c_embedding is not None:
                    similarity = cosine_similarity(
                        [a_embedding],
                        [c_embedding]
                    )[0][0]
                    scores.append((similarity + 1) / 2)
            
            # Retourner la moyenne des scores
            if scores:
                return float(np.mean(scores))
            return 0.0
        except Exception as e:
            print(f"Erreur dans context_recall : {e}")
            return 0.0
    
    def evaluate(self, question, answer, contexts):
        """
        Effectue une √©valuation compl√®te.
        Retourne un dictionnaire avec tous les scores.
        """
        return {
            "answer_relevancy": self.answer_relevancy(question, answer),
            "context_precision": self.context_precision(question, contexts),
            "context_recall": self.context_recall(answer, contexts)
        }

def main():
    print("ü§ñ llama-3.3-70b-versatile avec LlamaIndex RAG - √âvaluation Robuste")
    print("="*60)

    # V√©rifier la cl√© API Groq
    if not GROQ_API_KEY:
        print("‚ùå Erreur : GROQ_API_KEY non trouv√©e dans les variables d'environnement")
        return

    # Configurer les mod√®les LlamaIndex
    embed_model = OllamaEmbedding(model_name="bge-m3")
    Settings.embed_model = embed_model
    Settings.chunk_size = 512
    Settings.chunk_overlap = 20
    Settings.llm = Groq(model="llama-3.3-70b-versatile", api_key=GROQ_API_KEY)
    print("‚úÖ Mod√®les configur√©s !")

    # Cr√©er l'√©valuateur personnalis√©
    evaluator = CustomEvaluator(embed_model)
    print("‚úÖ √âvaluateur cr√©√© !")

    # Chargement des documents
    print("üìÑ Chargement des documents...")
    data = load_scraped_data()
    if not data:
        print("‚ùå Aucun document trouv√©. Assurez-vous que scraped_data.json existe.")
        return

    documents = create_documents(data)
    print(f"‚úÖ {len(documents)} documents charg√©s !")

    # Cr√©ation ou chargement de l'index persistant
    print("üîç Cr√©ation/Chargement de l'index...")
    index_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), '../data/vector_index')
    if os.path.exists(index_dir):
        from llama_index.core import StorageContext, load_index_from_storage
        storage_context = StorageContext.from_defaults(persist_dir=index_dir)
        index = load_index_from_storage(storage_context)
        print("‚úÖ Index charg√© depuis le disque !")
    else:
        index = VectorStoreIndex.from_documents(documents)
        index.storage_context.persist(persist_dir=index_dir)
        print("‚úÖ Index cr√©√© et sauvegard√© !")

    # Cr√©er le query engine et retriever
    query_engine = index.as_query_engine(similarity_top_k=3)
    retriever = index.as_retriever(similarity_top_k=3)

    # Chat interactif avec RAG + √©valuation personnalis√©e
    print("\n" + "="*60)
    print("üí¨ CHAT RAG - Posez vos questions sur le diab√®te !")
    print("="*60)
    print("‚Ä¢ Tapez votre question et appuyez sur Entr√©e")
    print("‚Ä¢ Tapez 'quit' pour quitter")
    print("-"*60)

    evaluation_results = []

    while True:
        try:
            user_question = input("\nüßë Question: ").strip()

            if user_question.lower() in ['quit', 'exit']:
                print("üëã Au revoir !")
                break

            if not user_question:
                continue

            print("\nüîç Recherche dans les documents...")
            
            # R√©cup√©rer la r√©ponse
            print("ü§ñ G√©n√©ration de la r√©ponse...")
            response = query_engine.query(user_question)
            response_text = str(response)
            
            print(f"\nü§ñ R√©ponse: {response_text}")

            # R√©cup√©rer les contextes pour l'√©valuation
            retrieved_docs = retriever.retrieve(user_question)
            contexts = [doc.text for doc in retrieved_docs]

            # √âvaluation personnalis√©e - RAPIDE ET FIABLE
            print("\nüìä √âvaluation en cours...")
            scores = evaluator.evaluate(user_question, response_text, contexts)
            
            # Afficher les r√©sultats
            print(f"\n‚úÖ R√©sultats d'√©valuation:")
            print(f"   ‚Ä¢ Answer Relevancy (Pertinence) : {scores.get('answer_relevancy', 0):.2f}")
            print(f"   ‚Ä¢ Context Precision (Pr√©cision contexte) : {scores.get('context_precision', 0):.2f}")
            print(f"   ‚Ä¢ Context Recall (Rappel contexte) : {scores.get('context_recall', 0):.2f}")
            
            # Score global
            global_score = np.mean([
                scores.get('answer_relevancy', 0),
                scores.get('context_precision', 0),
                scores.get('context_recall', 0)
            ])
            print(f"üìà Score global : {global_score:.2f}")
            
            # Sauvegarder les r√©sultats
            evaluation_results.append({
                "question": user_question,
                "scores": scores,
                "global_score": global_score
            })

        except KeyboardInterrupt:
            print("\n\nüëã Au revoir !")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur : {e}")
            import traceback
            traceback.print_exc()

    # Afficher un r√©sum√© des √©valuations
    if evaluation_results:
        print("\n" + "="*60)
        print("üìä R√âSUM√â DES √âVALUATIONS")
        print("="*60)
        avg_relevancy = np.mean([r['scores']['answer_relevancy'] for r in evaluation_results])
        avg_precision = np.mean([r['scores']['context_precision'] for r in evaluation_results])
        avg_recall = np.mean([r['scores']['context_recall'] for r in evaluation_results])
        
        print(f"Moyenne Answer Relevancy : {avg_relevancy:.2f}")
        print(f"Moyenne Context Precision : {avg_precision:.2f}")
        print(f"Moyenne Context Recall : {avg_recall:.2f}")
        print(f"Score global moyen : {np.mean([r['global_score'] for r in evaluation_results]):.2f}")


if __name__ == "__main__":
    main()