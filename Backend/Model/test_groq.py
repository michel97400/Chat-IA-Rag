import json
import os
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core import VectorStoreIndex, Document, Settings
from llama_index.llms.groq import Groq
from dotenv import load_dotenv

# Charger les variables d'environnement
load_dotenv()

# Configuration de l'API key (utilise la variable d'environnement si disponible)
GROQ_API_KEY = os.getenv("GROQ_API_KEY")




def load_scraped_data(json_file='data/scraped_data.json'):
    """
    Charge les donn√©es scrap√©es depuis le fichier JSON.
    """
    if not os.path.exists(json_file):
        print(f"Fichier {json_file} non trouv√©.")
        return []
    
    with open(json_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def create_documents(data):
    """
    Cr√©e des objets Document √† partir des donn√©es scrap√©es.
    """
    documents = []
    for item in data:
        doc = Document(
            text=item['content'],
            metadata={
                'url': item['url'],
                'timestamp': item['timestamp']
            }
        )
        documents.append(doc)
    return documents

def main():
    print("ü§ñ llama-3.3-70b-versatile avec LlamaIndex RAG - Version Simplifi√©e")
    print("="*60)

    # V√©rifier la cl√© API
    if not GROQ_API_KEY:
        print("‚ùå Erreur : GROQ_API_KEY non trouv√©e dans les variables d'environnement")
        return

    # Configurer les mod√®les
    embed_model = OllamaEmbedding(model_name="bge-m3")
    Settings.embed_model = embed_model
    Settings.chunk_size = 512
    Settings.chunk_overlap = 20

    print("‚úÖ Mod√®les configur√©s !")

    # Chargement des documents
    print("üìÑ Chargement des documents...")
    data = load_scraped_data()
    if not data:
        print("‚ùå Aucun document trouv√©. Assurez-vous que scraped_data.json existe.")
        return

    documents = create_documents(data)
    print(f"‚úÖ {len(documents)} documents charg√©s !")

    # Cr√©ation de l'index
    print("üîç Cr√©ation de l'index...")
    index = VectorStoreIndex.from_documents(documents)
    print("‚úÖ Index cr√©√© !")

    # R√©cup√©rateur de documents
    retriever = index.as_retriever(similarity_top_k=3)

    # Initialiser le LLM Groq
    llm = Groq(model="llama-3.3-70b-versatile", api_key=GROQ_API_KEY)

    # Chat interactif avec RAG
    print("\n" + "="*60)
    print("üí¨ CHAT RAG - Posez vos questions sur le diab√®te !")
    print("="*60)
    print("‚Ä¢ Tapez votre question et appuyez sur Entr√©e")
    print("‚Ä¢ Tapez 'quit' pour quitter")
    print("-"*60)

    while True:
        try:
            user_question = input("\nüßë Question: ").strip()

            if user_question.lower() in ['quit', 'exit']:
                print("üëã Au revoir !")
                break

            if not user_question:
                continue

            print("\nüîç Recherche dans les documents...")

            # R√©cup√©ration des documents pertinents
            retrieved_docs = retriever.retrieve(user_question)

            # Construction du contexte
            context_text = ""
            for doc in retrieved_docs:
                context_text += f"{doc.text[:500]}...\n\n"  # Limiter la longueur

            # Construction du prompt
            prompt = f"""Contexte: Voici des informations pertinentes trouv√©es dans la documentation :

{context_text}

Question: {user_question}

R√©ponse: En me basant sur les informations du contexte ci-dessus, """

            print("ü§ñ G√©n√©ration de la r√©ponse...")

            response = llm.complete(prompt)

            # Nettoyer la r√©ponse
            if "R√©ponse:" in response:
                response = response.split("R√©ponse:")[-1].strip()

            print(f"\nü§ñ R√©ponse: {response}")

        except KeyboardInterrupt:
            print("\n\nüëã Au revoir !")
            break
        except Exception as e:
            print(f"\n‚ùå Erreur : {e}")


if __name__ == "__main__":
    main()